{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentRegressionSKL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0-7LEXrh4le"
      },
      "source": [
        "####IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOyxoI_6hxFV"
      },
      "source": [
        "import ntpath\n",
        "from os import walk, path\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "# PREPROCESSING related imports\n",
        "\n",
        "from random import shuffle\n",
        "import re    \n",
        "import string  \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples                          \n",
        "from nltk.stem import PorterStemmer        \n",
        "from nltk.tokenize import TweetTokenizer   \n",
        "from nltk.corpus import stopwords          \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzSZkolnh8nw"
      },
      "source": [
        "dir = '.'\n",
        "data_p = \"/content/\"\n",
        "train_p = data_p + 'Dataset/train/pos/' \n",
        "train_n = data_p + 'Dataset/train/neg/'\n",
        "\n",
        "def download_data():\n",
        "  !gdown --id 1pwbVIT5yyUbQZTKp6Fy3gir9eOsUj3nB --output \"{dir}/data.zip\"\n",
        "\n",
        "def unzip_data():\n",
        "  f = dir+\"/data.zip\"\n",
        "  !mkdir \"$data_p\"\n",
        "  !unzip \"$f\" -d \"$data_p\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPC-rXuUiNK7"
      },
      "source": [
        "def getF(directory, x=\"both\"):\n",
        "    # x could be \"files\" or \"folders\" or \"both\"\n",
        "\n",
        "    folders = []\n",
        "    for f in walk(directory):\n",
        "        folders.extend(f)\n",
        "\n",
        "    if x == \"folders\":\n",
        "\n",
        "      return folders[1]\n",
        "      N\n",
        "      \n",
        "        \n",
        "    if x == \"files\":\n",
        "        return folders[2]\n",
        "    else:\n",
        "      return folders[1], folders[2]\n",
        "\n",
        "\n",
        "\n",
        "# read all text files\n",
        "\n",
        "def read_Alltxt(read_path):\n",
        "  files = getF(read_path,'files')\n",
        "  txtFiles = []\n",
        "  for f in files:\n",
        "    with open(read_path+f, 'r', encoding=\"utf-8\") as of:\n",
        "      data = ''\n",
        "      for line in of:\n",
        "        data += line\n",
        "      txtFiles.append(data)\n",
        "  return txtFiles\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def BOW(f_as_ftokens, labels):\n",
        "    bow = {}\n",
        "\n",
        "    for tokens, label in list(zip(f_as_ftokens, labels)):\n",
        "        for token in tokens:\n",
        "            bow[(token, label)] = bow.get((token, label), 0) + 1\n",
        "        \n",
        "    return bow\n",
        "\n",
        "\n",
        "\n",
        "# feature vectors\n",
        "def extract_features(f_as_ftokens, bow):\n",
        "    # feature array\n",
        "    features = np.zeros((1,3))\n",
        "    # bias term added in the 0th index\n",
        "    features[0,0] = 1\n",
        "    \n",
        "    # iterate processed_tweet\n",
        "    for word in f_as_ftokens:\n",
        "        # get the positive frequency of the word\n",
        "        features[0,1] = bow.get((word, 1), 0)\n",
        "        # get the negative frequency of the word\n",
        "        features[0,2] = bow.get((word, 0), 0)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOGL3m6xiYgB"
      },
      "source": [
        "download_data()\n",
        "unzip_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKyKhUgsikHe"
      },
      "source": [
        "data_p = read_Alltxt(train_p)\n",
        "\n",
        "data_n = read_Alltxt(train_n)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q8qCk_tinim"
      },
      "source": [
        "class Preprocessor():\n",
        "    def __init__(self):\n",
        "      self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                                      reduce_len=True)\n",
        "      nltk.download('stopwords')\n",
        "      self.stopwords_en = stopwords.words('english') \n",
        "      self.punctuation_en = string.punctuation\n",
        "      self.stemmer = PorterStemmer() \n",
        "\n",
        "    def __clean_n_tokenize_text__(self, txt):\n",
        "\n",
        "        # remove hyperlinks\n",
        "        txt = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', txt)\n",
        "        # remove hashtags\n",
        "        txt = re.sub(r'#', '', txt)\n",
        "        #remove email address\n",
        "        txt = re.sub('\\S+@\\S+', '', txt)\n",
        "        # remove numbers\n",
        "        txt = re.sub(r'\\d+', '', txt)\n",
        "        txt =  self.tokenizer.tokenize(txt)\n",
        "        return txt\n",
        "\n",
        "\n",
        "    def __filter_stopwords__(self, tokens):\n",
        "        # remove stopwords\n",
        "        filtered_tokens = []\n",
        "\n",
        "        for word in tokens:\n",
        "            if (word not in self.stopwords_en and  # remove stopwords\n",
        "                word not in self.punctuation_en):  # remove punctuation\n",
        "                filtered_tokens.append(word)\n",
        "        return filtered_tokens\n",
        "\n",
        "    def __stem_tokens__(self, tokens):\n",
        "        # store the stemmed word\n",
        "        stemmed_tokens = [] \n",
        "\n",
        "        for word in tokens:\n",
        "            stem_word = self.stemmer.stem(word)  \n",
        "            stemmed_tokens.append(stem_word)\n",
        "        return stemmed_tokens\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess(self, txts):\n",
        "        final_tokens = []\n",
        "        for _, txt in tqdm(enumerate(txts)):        \n",
        "            txt = self.__clean_n_tokenize_text__(txt)                       \n",
        "            txt = self.__filter_stopwords__(txt)\n",
        "            txt = self.__stem_tokens__(txt)\n",
        "            final_tokens.extend([txt])\n",
        "        return final_tokens"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90A_CKj1isOJ",
        "outputId": "b6fcf17b-d0c4-407a-b520-63093797dc15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "processor = Preprocessor()\n",
        "\n",
        "# process the positive and negative tweets\n",
        "pfs_as_ftokens = processor.preprocess(data_p)\n",
        "nfs_as_ftokens = processor.preprocess(data_n)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24it [00:00, 228.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12500it [00:52, 236.97it/s]\n",
            "12500it [00:52, 237.64it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t553TX_IiyBy"
      },
      "source": [
        "# Labels\n",
        "\n",
        "labels = [1 for i in range(len(pfs_as_ftokens))]\n",
        "labels.extend([0 for i in range(len(nfs_as_ftokens))])\n",
        "pos_neg_fs_as_ftoken = pfs_as_ftokens + nfs_as_ftokens"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VPVMB1li1TP"
      },
      "source": [
        "bow = BOW(pos_neg_fs_as_ftoken, labels)\n",
        "df = pd.DataFrame(list(zip(pos_neg_fs_as_ftoken, labels)), columns=[\"f_as_tokens\", \"labels\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwE-MaaWi6zh"
      },
      "source": [
        "train_Xt, test_Xt, train_Y, test_Y = train_test_split(df[\"f_as_tokens\"], df[\"labels\"], test_size = 0.2, shuffle=True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJeuZFlWi8t4"
      },
      "source": [
        "train_X = np.zeros((len(train_Xt), 3))\n",
        "\n",
        "for index, f_as_token in enumerate(train_Xt):\n",
        "    train_X[index, :] = extract_features(f_as_token, bow)\n",
        "\n",
        "# test X feature dimension\n",
        "test_X = np.zeros((len(test_Xt), 3))\n",
        "\n",
        "for index, f_as_token in enumerate(test_Xt):\n",
        "    test_X[index, :] = extract_features(f_as_token, bow)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg13RVAxjFc4",
        "outputId": "d5c1b8a0-2de6-4a6b-fcfb-d645a6787b65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "classifier = LogisticRegression(random_state=0)\n",
        "classifier.fit(train_X, train_Y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLj59kqcjHvz"
      },
      "source": [
        "predictions = classifier.predict(test_X)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGfD7slMjN43",
        "outputId": "c203b2e0-aefd-4d34-d3c7-aebb6d5a676d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "print(confusion_matrix(test_Y,predictions))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1293 1181]\n",
            " [ 747 1779]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-JkyCc9jPnM",
        "outputId": "27c6223d-9722-4a51-af2d-59174b082944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(classification_report(test_Y,predictions))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.52      0.57      2474\n",
            "           1       0.60      0.70      0.65      2526\n",
            "\n",
            "    accuracy                           0.61      5000\n",
            "   macro avg       0.62      0.61      0.61      5000\n",
            "weighted avg       0.62      0.61      0.61      5000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs-_BVJ5jRMd",
        "outputId": "96d57c4a-370b-45f7-b88f-d5ebe675eef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(accuracy_score(test_Y, predictions))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6144\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}